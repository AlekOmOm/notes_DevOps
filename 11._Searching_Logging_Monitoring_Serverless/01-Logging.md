# 01. Logging Systems üìù

[<- Back to Main Note](./README.md) | [Next: Monitoring ->](./02-Monitoring.md)

## Table of Contents

- [Introduction](#introduction)
- [Logging vs. Related Concepts](#logging-vs-related-concepts)
- [Why Log?](#why-log)
- [Logging Levels](#logging-levels)
- [Logging Formats](#logging-formats)
- [Where to Log](#where-to-log)
- [Logging Stacks](#logging-stacks)
- [Best Practices](#best-practices)

## Introduction

Logging is the systematic activity of collecting and analyzing data generated by applications, infrastructure, and other components of a system. In essence, logging provides streams of aggregated, time-ordered events collected from running processes.

Effective logging is a cornerstone of modern software systems, providing visibility into application behavior, aiding in troubleshooting, and supporting compliance requirements.

## Logging vs. Related Concepts

Before diving deeper, it's important to understand how logging fits with other observability practices:

| Concept | Focus | Purpose |
|---------|-------|---------|
| **Logging** | Record discrete events | Debugging, audit trails, historical data |
| **Monitoring** | System-wide metrics observation | Operational awareness, alerts, trends |
| **Tracing** | Request journey across services | End-to-end request flow analysis |
| **Profiling** | Performance measurement | Identifying bottlenecks, resource usage |

## Why Log?

Logging serves multiple critical purposes:

1. **Diagnosis**: When something goes wrong, logs help identify what happened, where, and why
2. **Anomaly detection**: Statistical analysis of logs can identify outliers and unusual patterns
3. **Understanding usage**: Logs provide insights into how users interact with your system
4. **Security**: Detecting unauthorized access attempts or suspicious activities
5. **Compliance**: Meeting legal requirements by creating audit trails

Logging is also big business - companies like LogPoint, Splunk, and others have built billion-dollar enterprises around log management and analysis.

## Logging Levels

Standard logging levels provide a way to categorize the severity and importance of log messages:

```python
import logging

logging.basicConfig(level=logging.INFO)

logging.debug("Detailed information for debugging purposes")     # Won't be shown
logging.info("Confirmation that things are working as expected") # Will be shown
logging.warning("An indication something unexpected happened")   # Will be shown
logging.error("The software couldn't perform some function")     # Will be shown
logging.critical("A serious error, program may be unable to continue") # Will be shown
```

| Level | Description | When to Use |
|-------|-------------|-------------|
| **DEBUG** | Detailed information for troubleshooting | During development or when investigating specific issues |
| **INFO** | Confirmation that things are working as expected | For important application events and milestones |
| **WARNING** | Something unexpected happened, but the application can continue | For potential issues that don't prevent operation |
| **ERROR** | A function couldn't complete its task | When something fails but the application can still run |
| **FATAL/CRITICAL** | A serious error that may prevent the program from continuing | For catastrophic failures |

When you set a logging level, you'll see messages at that level and all higher-severity levels:

| Logging Level Set | Displays Messages at Levels |
|---------------|-----------------------------|
| **DEBUG**     | DEBUG, INFO, WARNING, ERROR, CRITICAL |
| **INFO**      | INFO, WARNING, ERROR, CRITICAL |
| **WARNING**   | WARNING, ERROR, CRITICAL     |
| **ERROR**     | ERROR, CRITICAL              |
| **CRITICAL**  | CRITICAL only                 |

## Logging Formats

Standardizing log formats ensures consistency and easier processing:

### Syslog Format

The Syslog protocol (defined in RFC 3164/5424) standardizes log formatting and transmission:

```
<PRI>TIMESTAMP HOSTNAME APP-NAME PROCID MSGID STRUCTURED-DATA MSG
```

Example implementation in Python:

```python
import sys
import logging
import socket

logging.basicConfig(
    format="%(asctime)s %(hostname)s %(module)s[%(process)d]: %(levelname)s %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    level=logging.INFO,
    stream=sys.stdout
)

# Add hostname dynamically
logging.Logger.adapter = logging.LoggerAdapter(
    logging.getLogger(), 
    {'hostname': socket.gethostname()}
)

logging.info('User updated their profile')
```

### JSON Logging

For more structured logging, especially in cloud environments:

```javascript
const winston = require('winston');

const logger = winston.createLogger({
  level: 'info',
  format: winston.format.json(),
  defaultMeta: { service: 'user-service' },
  transports: [
    new winston.transports.File({ filename: 'error.log', level: 'error' }),
    new winston.transports.File({ filename: 'combined.log' })
  ]
});

logger.info('User profile updated', {
  userId: '123',
  changes: ['email', 'name'],
  timestamp: new Date().toISOString()
});
```

## Where to Log

Different logging destinations serve different purposes:

### Local Filesystem

- Standard location on Linux: `/var/log/`
- Pros: Simple, no external dependencies
- Cons: Limited to a single server, requires log rotation

### Centralized Logging Servers

- Dedicated logging infrastructure
- Common solutions: ELK stack, Graylog, Splunk
- Pros: Aggregation across services, advanced search
- Cons: Additional infrastructure, complexity

### Cloud Logging Services

- Managed logging solutions: AWS CloudWatch, Google Cloud Logging
- Pros: Scalable, integrated with cloud services
- Cons: Vendor lock-in, potential costs

## Logging Stacks

Several popular stacks exist for comprehensive logging:

### ELK Stack

- **ElasticSearch**: Scalable full-text search database
- **Logstash**: Log parser and enrichment pipeline
- **Kibana**: Visualization and dashboard tool

### EFLK Stack

- Adds **Filebeat** for lightweight log shipping

### EFK Stack

- Replaces Logstash with **Fluentd** for unified logging

### Graylog

- Combines Graylog server, MongoDB, and Elasticsearch

### Loki + Grafana

- Lightweight, Prometheus-inspired logging stack

## What Not to Log

Certain types of data should never be logged:

- **Passwords** and authentication credentials
- **Credit card numbers** and financial data
- **Personal identifiable information** (PII)
- **Health information** protected by regulations
- **Session tokens** and security keys

Instead, create reference IDs for sensitive data and store the actual data elsewhere, following the principle of separation of concerns.

## Best Practices

1. **Use structured logging**: JSON or other structured formats make logs easier to parse and analyze
   
2. **Include context**: Add relevant details like request IDs, user IDs, and timestamps
   
3. **Be consistent**: Use standardized formats and levels across your application
   
4. **Implement log rotation**: Prevent logs from consuming all available disk space
   
5. **Consider security**: Never log sensitive information; use reference IDs instead
   
6. **Plan for scale**: Design your logging system to handle production volumes
   
7. **Use correlation IDs**: Track requests across distributed systems
   
8. **Optimize verbosity**: Use appropriate log levels to avoid overwhelming your systems

For more detailed implementation specifics, see:
- [01a. Logging Implementation](./01a-Logging-Implementation.md)
- [01b. Logging in Docker](./01b-Logging-in-Docker.md)

---

[<- Back to Main Note](./README.md) | [Next: Monitoring ->](./02-Monitoring.md)
